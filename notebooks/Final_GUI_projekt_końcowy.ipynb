{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopia notatnika GUI_projekt_końcowy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZDDATApol4/prediction-political-preferences/blob/main/Final_GUI_projekt_ko%C5%84cowy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "VZ1wCL1DbIi3"
      },
      "source": [
        "#@title <----- Najpierw uruchom to\n",
        "\n",
        "%%capture\n",
        "!git clone https://github.com/ZDDATApol4/prediction-political-preferences\n",
        "\n",
        "!pip install spacy==3\n",
        "!python -m spacy download pl_core_news_sm\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import time\n",
        "import keras\n",
        "import spacy\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from keras.models import load_model\n",
        "from plotly.offline import iplot\n",
        "\n",
        "# Twitter_Credentials\n",
        "\n",
        "consumer_key = \"5MzB6K9I3B0hVF0BOs2UjybSz\"\n",
        "consumer_secret = \"hiPjP10G7bTuomHflC1vCWzwcdsAB444WkE0MkSZwGGfALoxBS\"\n",
        "access_token = \"1437020925040513029-s6Dqtv9lPyufnlKtzm9tF3BQEQ8FCA\"\n",
        "access_token_secret = \"D9SUsW12b1ya9LuWPNNmGmU0NhKIJzoeFfHsIy4xgdJ7x\"\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "\n",
        "tweets = []\n",
        "\n",
        "def username_tweets_to_csv(username,count):\n",
        "    try:      \n",
        "        # Creation of query method using parameters\n",
        "        tweets = tweepy.Cursor(api.user_timeline,id=username).items(count)\n",
        "\n",
        "        # Pulling information from tweets iterable object\n",
        "        tweets_list = [[tweet.user.screen_name, tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
        "\n",
        "        # Creation of dataframe from tweets list\n",
        "        tweets_df = pd.DataFrame(tweets_list,columns=['Author', 'Datetime', 'Tweet Id', 'Text'])\n",
        "                      \n",
        "        return tweets_df\n",
        "\n",
        "    except BaseException as e:\n",
        "          print('failed on_status,',str(e))\n",
        "          time.sleep(3)\n",
        "\n",
        "def lemmatize_text(text_corpus):\n",
        "  lemmatized_text = []\n",
        "  for tweet in tqdm(text_corpus):\n",
        "    doc = nlp(tweet)\n",
        "    doc_lemmatized = []\n",
        "    for x in doc:\n",
        "      if x.is_punct == False and x.is_stop == False:\n",
        "        doc_lemmatized.append(x.lemma_)\n",
        "    lemmatized_text.append(doc_lemmatized)\n",
        "  return lemmatized_text\n",
        "\n",
        "def tokenize(num_words, lemmatized_text, maxlen):\n",
        "  tokenizer = Tokenizer(num_words=num_words)\n",
        "  tokenizer.fit_on_texts(lemmatized_text)\n",
        "  sequences = tokenizer.texts_to_sequences(lemmatized_text)\n",
        "  padded = pad_sequences(sequences, maxlen=maxlen)\n",
        "  print(padded.shape)\n",
        "  return padded\n",
        "\n",
        "nlp = spacy.load('pl_core_news_sm')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABV3vFHpX-GQ",
        "cellView": "form"
      },
      "source": [
        "#@title Wpisz Twitter ID i naciśnij PLAY\n",
        "\n",
        "ID = \"\" #@param [\"\", \"\"] {allow-input: true}\n",
        "\n",
        "count = 1000\n",
        "data = username_tweets_to_csv(ID, count)\n",
        "\n",
        "#Obróbka danych\n",
        "\n",
        "data = data[data['Datetime'] >= pd.to_datetime('2021-08-01')]\n",
        "test_text_corpus = data['Text']\n",
        "\n",
        "# Remove @replies from source and target text\n",
        "regex = r'(@\\w+\\s)+'\n",
        "test_text_corpus = test_text_corpus.apply(lambda x: re.sub(regex, '', x))\n",
        "\n",
        "# Remove URLS from source and target text\n",
        "regex = r'https?:\\/\\/.*[\\r\\n]*'\n",
        "test_text_corpus = test_text_corpus.apply(lambda x: re.sub(regex, '', x, flags=re.MULTILINE))\n",
        "\n",
        "# convert source and target text to Lowercase \n",
        "test_text_corpus = test_text_corpus.apply(lambda x: x.lower())\n",
        "\n",
        "# Remove quotes from source and target text\n",
        "test_text_corpus = test_text_corpus.apply(lambda x: re.sub(\"'\", '', x))\n",
        "\n",
        "# create a set of all special characters\n",
        "special_characters= set(string.punctuation)\n",
        "# Remove all the special characters\n",
        "test_text_corpus = test_text_corpus.apply(lambda x: ''.join(char1 for char1 in x if char1 not in special_characters))\n",
        "\n",
        "# Remove digits from source and target sentences\n",
        "num_digits= str.maketrans('','', digits)\n",
        "test_text_corpus = test_text_corpus.apply(lambda x: x.translate(num_digits))\n",
        "\n",
        "#Lematyzacja\n",
        "test_lemmatized = lemmatize_text(test_text_corpus)\n",
        "\n",
        "#Padding\n",
        "test_padded = tokenize(num_words=50000, lemmatized_text=test_lemmatized, maxlen=20)\n",
        "\n",
        "#Wczytanie_modelu\n",
        "loaded_model = load_model('prediction-political-preferences/test_model_for_dash.h5')\n",
        "\n",
        "predicted = loaded_model.predict(test_padded)\n",
        "\n",
        "df = pd.DataFrame(predicted, columns=['KO', 'Konfederacja', 'Lewica', 'PSL', 'PiS'])\n",
        "\n",
        "#Wykres\n",
        "pd.plotting.register_matplotlib_converters()\n",
        "\n",
        "pie = df.mean()\n",
        "labels = ['KO', 'Konfederacja', 'Lewica', 'PSL', 'PiS']\n",
        "fig = {\n",
        "    'data': [\n",
        "      {\n",
        "       'values': pie,\n",
        "       'labels': labels,\n",
        "       'domain': {'x': [0, .5]},\n",
        "       'name': '',\n",
        "       'hoverinfo': 'label+percent+name',\n",
        "       'hole': .3,\n",
        "       'type': 'pie'\n",
        "       }, ],\n",
        "     'layout': {\n",
        "         'legend_x': 0.5,\n",
        "         'legend_y': 0.5,\n",
        "         'title': f'Preferencje polityczne użytkownika \"{ID}\" na podstawie tweetów',\n",
        "         'title_x': 0.06,\n",
        "         'title_y': 0.9,\n",
        "     }  \n",
        "}\n",
        "iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}